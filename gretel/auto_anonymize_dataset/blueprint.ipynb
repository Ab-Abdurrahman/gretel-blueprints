{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -Uqq gretel-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-anonymize production datasets for development\n",
    "\n",
    "Data seeded in development, test, and other pre-production environments often don't have parity with production data. This difference in quality makes it difficult to track down bugs during development, and often leads to bugs that only occur in production.\n",
    "\n",
    "In this blueprint, we take a production dataset containing sensitive, personally identifying details and generate a fake, anonymized copy of that dataset. The resulting dataset has the same shape, and can be loaded into pre-production databases, but isn't re-identifiable back to any customer.\n",
    "\n",
    "Using Gretel's [Data Catalog](https://gretel.ai/platform/data-catalog) and [Transformation](https://gretel.ai/platform/transform) tools we walk-through a notebook that will analyze a source dataset and automatically generate a data pipeline that will transform a production dataset. While this demonstration runs as a notebook, this same pipeline can be deployed into a variety of different data stacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "First we'll import Gretel package depedencies and instantiate a client pointing to the newly created project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_client import project_from_uri\n",
    "\n",
    "project = project_from_uri(\"prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.client.install_packages(version=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect source dataset\n",
    "\n",
    "For this demonstration we've chosen a dataset containing bicycle order details. The dataset contains identifying information such as names, email and individual financial details. Gretel's Data Catalog will extract entities such as names, emails and locations using custom pattern matching and machine learning based NLP models. We'll use these entities to help determine what fields need to be anonymized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the pipeline\n",
    "\n",
    "`gretel_auto_xf` is a package built by Gretel that helps build transformation pipelines. The package uses a set of rules and heuristics to automically generate transformations based on the contents and metadata of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gretel_auto_xf.pipeline import build_pipeline\n",
    "from gretel_auto_xf.helpers import rule_inspector, df_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`build_pipeline` will analyze the source dataset and generate a transformation pipeline that can be used to create an anonymized version of the source dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = build_pipeline(project, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After analyzing the dataset, a set of rules are matched and automatically collected into a pipeline. Using `rule_inspector` you may select or deselect rules based on your specific requirements or privacy constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rule_inspector(pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the anonymization pipeline\n",
    "\n",
    "Now that we've selected what transformations to apply, we can run the pipeline against the Gretel project. `xf_project` will retrieve the original records from the Gretel project and apply the anonymization pipeline.\n",
    "\n",
    "The result of `xf_records` is an anonymized version of the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_df = pipeline.xf_project(as_df=True, show_progress=True, batch_pipeline=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare datasets\n",
    "\n",
    "Let's compare the two datasets... `df_diff` will perform a row-wise comparison by field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_diff(project.head(), anonymized_df, index=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the anonymized dataset\n",
    "\n",
    "Now that we've generated an anonymized version of the dataset, let's save it so it can be shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "anonymized_df.to_csv(\"bike_orders_anonymized.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can upload the anonymized dataset to Gretel where it can be safely accessed by other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_xf = project.client.get_project(display_name=\"Anonymized Bike Orders\", create=True)\n",
    "project_xf.send_dataframe(anonymized_df, use_progress_widget=True)\n",
    "\n",
    "print(f\"Your new Gretel project has been created! Access it here, {project_xf.get_console_url()}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
